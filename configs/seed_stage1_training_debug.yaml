cfg_path: ???
tokenizer_cfg_path: configs/tokenizer/seed_llama_tokenizer_hf.yaml
transform_cfg_path: configs/transform/clip_transform.yaml
model_cfg_path: configs/llm/seed_llama_8b.yaml 
result_path: ???
result_file_path: ./logs/seed_stage_1_training_debug
checkpoint_path:
  model_path: pretrained/seed_tokenizer/seed_quantizer.pt
  diffusion_model_path: stabilityai/stable-diffusion-2-1-unclip
karpathy_file_path: /ssd0/data/coco/annotations/karpathy/dataset_coco_test.json
root_dir: /ssd0/data/coco/images/val2014
eval: False
resume: False
finetune: False

dist:
  n_gpus: 4
  n_nodes: 1

dataset:
  type: webdataset
  # name: cc3m_coco_laion_coco2
  name: cc15m_3m_laion400M_karpathy
  loc: bcloud
  num_workers: 64
  shuffle: True
  tokenizer:
    type: ClipBPE
    hparams:
      context_length: 256
      bpe_pdrop: 0.1
  transform:
    type: dalle-vqvae
    hparams:
      resolution: 224
  gt_text: True

stage1:
  ema_update: False

stage2:
  image_caption_ratio: 0.5

experiment:
  seed: 0
  # local_batch_size: 256
  local_batch_size: 310
  # local_batch_size: 290 
  val_batch_size: 256 
  test_split: train
  max_epochs: 20
  grad_accumulation: 1
  # overfit_batches: 1
  # val_check_interval: 100
  check_val_every_n_epoch: 1
  enable_checkpointing: True
  log_every_n_steps: 1
  num_sanity_val_steps: 2

optimizer:
  # Choose between 'bf16', 'fp32', 'fp16'
  fp16: True
  precision: 'bf16'
  max_lr: 5e-5
  # grad clip value zero is equivalent to no clipping
  grad_clip_val: 0

hyperparameters:
  beta_1: 0.9
  beta_2: 0.98
  weight_decay: 1e-8

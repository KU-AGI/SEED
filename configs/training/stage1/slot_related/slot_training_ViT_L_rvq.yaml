cfg_path: ???
tokenizer_cfg_path: configs/tokenizer/seed_llama_tokenizer_hf.yaml
# transform_cfg_path: configs/transform/clip_transform.yaml
transform_cfg_path: configs/transform/slot_transform.yaml
model_cfg_path: configs/llm/seed_llama_8b.yaml 
result_file_path: ./logs/slot_diffusion
# result_file_path: ./logs/seed_slot_layer1
checkpoint_path:
  model_path: pretrained/seed_tokenizer/seed_quantizer.pt
  diffusion_model_path: stabilityai/stable-diffusion-2-1-unclip
# slot_cfg_path: configs/slot_attn/config_12heads_dino.json
slot_cfg_path: configs/slot_attn/config_12heads_dino_vit_L.json

resume: False
load_weight: True
# weight_path:
weight_path: logs/slot_stage1_related/slot_diffusion_dino_12head_dataaug_notlearnablequery_200epoch_crossattn_unfreeze_vitL_data_increase_unfreeze_unet_causal_v2/checkpoints/epoch=90-step=75477.ckpt
# weight_path: pretrained/seed_slot_layer1_dino_12head_dataaug_learnablequery_200epoch_crossattn_unfreeze/checkpoints/epoch=88-step=122926.ckpt
# weight_path: logs/slot_quantization_related/slot_vit_L_with_quantization_lfq_wo_blocks_5_2_10/checkpoints/epoch=13-step=6482.ckpt
#weight_path: logs/slot_quantization_related/residual_vq_train_blocks_image_unfreeze_unet_linear_4_blocks_4_blocks_image_32_embed_dim_16384_codebook_dim_4gpus_data_ratio_multi_head_8_shared_codebook_resume_6gpus/checkpoints/epoch=56-step=43013.ckpt
eval: False

dist:
  n_gpus: 6
  n_nodes: 1

dataset:
  train_config:
    # dataset_configs: ['configs/data/cc15m.yaml', 'configs/data/laion-coco.yaml', 'configs/data/mscoco.yaml']
    # weights: [1, 6, 4]
    dataset_configs: ['configs/data/cc15m-total.yaml', 'configs/data/laion-coco.yaml', 'configs/data/mscoco.yaml', 'configs/data/unsplash.yaml']
    weights: [15, 24, 1, 5]
    shardshuffle: 100
    resampled: True
    world_size: 1
    one_epoch_data_size: 1000000
  val_config:
    use_coco_val: True
    karpathy_file_path: coco/annotations/karpathy/dataset_coco_test.json
    root_dir: coco/images/val2014
    start_index: 0
    end_index: 256
  num_workers: 16
  shuffle: True
  text_max_length: 128

stage1:
  dino_model_name: 'dinov2_vitl14' # ['dinov2_vits14', 'dinov2_vitb14', 'dinov2_vitl14', 'dinov2_vitg14']
  unfreeze_unet: True
  image_size: 256 # [256, 512]
  causal_blocks_layers: 2

stage2:
  loss_weight:
    loss_codebook: 5
    loss_recon: 2
    loss_diffusion: 0.5
  blocks_layers: 4
  blocks_image_layers:
  use_blocks_image: False
  unclip: False
  vq:
    vq_type: 'residual_vq'
    codebook_embed_dim: 32
    n_embed: 8192
    num_quantizers: 8
  bypass_codebook: True

experiment:
  seed: 0
  stage: 2
  # local_batch_size: 16
  local_batch_size: 200
  val_batch_size: 16
  test_split: train
  max_epochs: 200
  deterministic: False
  grad_accumulation: 1
  val_check_interval: 0.5
  # check_val_every_n_epoch: 1 
  enable_checkpointing: True
  log_every_n_steps: 1
  num_sanity_val_steps: 1
  num_warmup_steps: 200
  grad_clip_val: 5
  find_unused_parameters: True

optimizer:
  vit_precision: 'fp16'
  diffusion_precision: 'fp16'
  precision: '16'
  max_lr: 5e-5

hyperparameters:
  beta_1: 0.9
  beta_2: 0.999
  weight_decay: 1e-2